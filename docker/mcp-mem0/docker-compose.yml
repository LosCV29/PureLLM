# MCP Memory Server for PureLLM
# Self-hosted mcp-mem0 with PostgreSQL + pgvector
#
# Usage:
#   1. Run: docker-compose up -d
#   2. Configure PureLLM to connect to http://localhost:8050
#
# Uses:
#   - vLLM (localhost:1234) for chat/memory processing
#   - Ollama (localhost:11434) for embeddings

services:
  # PostgreSQL with pgvector for semantic memory storage
  postgres:
    image: pgvector/pgvector:pg16
    container_name: mcp-mem0-postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: mem0
      POSTGRES_PASSWORD: mem0secret
      POSTGRES_DB: mem0
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U mem0"]
      interval: 5s
      timeout: 5s
      retries: 5

  # MCP Memory Server
  mcp-mem0:
    image: ghcr.io/coleam00/mcp-mem0:latest
    container_name: mcp-mem0-server
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    ports:
      - "8050:8050"
    environment:
      # Transport: SSE for HTTP-based MCP
      TRANSPORT: sse
      HOST: 0.0.0.0
      PORT: 8050

      # Database connection
      DATABASE_URL: postgresql://mem0:mem0secret@postgres:5432/mem0

      # LLM: Use vLLM (OpenAI-compatible) for memory processing
      LLM_PROVIDER: openai
      LLM_BASE_URL: http://host.docker.internal:1234/v1
      LLM_API_KEY: not-needed
      LLM_CHOICE: Qwen/Qwen2.5-VL-7B-Instruct

      # Embeddings: Use Ollama for semantic search
      EMBEDDING_PROVIDER: ollama
      EMBEDDING_MODEL_CHOICE: nomic-embed-text
      OLLAMA_BASE_URL: http://host.docker.internal:11434
    extra_hosts:
      # Allow container to reach host services (vLLM + Ollama)
      - "host.docker.internal:host-gateway"

volumes:
  postgres_data:
    driver: local
