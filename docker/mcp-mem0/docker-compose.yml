# MCP Memory Server for PureLLM
# Self-hosted mcp-mem0 with PostgreSQL + pgvector
#
# Usage:
#   1. Copy .env.example to .env and configure your settings
#   2. Run: docker-compose up -d
#   3. Configure PureLLM to connect to http://<host>:8050
#
# For Ollama (local embeddings): Make sure Ollama is running and accessible
# For OpenAI: Set your API key in the .env file

services:
  # PostgreSQL with pgvector for semantic memory storage
  postgres:
    image: pgvector/pgvector:pg16
    container_name: mcp-mem0-postgres
    restart: unless-stopped
    environment:
      POSTGRES_USER: ${POSTGRES_USER:-mem0}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-mem0secret}
      POSTGRES_DB: ${POSTGRES_DB:-mem0}
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-mem0}"]
      interval: 5s
      timeout: 5s
      retries: 5

  # MCP Memory Server
  mcp-mem0:
    image: ghcr.io/coleam00/mcp-mem0:latest
    container_name: mcp-mem0-server
    restart: unless-stopped
    depends_on:
      postgres:
        condition: service_healthy
    ports:
      - "${MCP_PORT:-8050}:8050"
    environment:
      # Transport: SSE for HTTP-based MCP
      TRANSPORT: sse
      HOST: 0.0.0.0
      PORT: 8050

      # Database connection
      DATABASE_URL: postgresql://${POSTGRES_USER:-mem0}:${POSTGRES_PASSWORD:-mem0secret}@postgres:5432/${POSTGRES_DB:-mem0}

      # LLM Provider for embeddings (choose one)
      LLM_PROVIDER: ${LLM_PROVIDER:-ollama}
      LLM_BASE_URL: ${LLM_BASE_URL:-http://host.docker.internal:11434}
      LLM_API_KEY: ${LLM_API_KEY:-}
      LLM_CHOICE: ${LLM_CHOICE:-llama3.2}

      # Embedding model
      EMBEDDING_MODEL_CHOICE: ${EMBEDDING_MODEL_CHOICE:-nomic-embed-text}
    extra_hosts:
      # Allow container to reach host's Ollama instance
      - "host.docker.internal:host-gateway"

volumes:
  postgres_data:
    driver: local
