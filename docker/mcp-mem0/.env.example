# MCP Memory Server Configuration
# Copy this file to .env and customize as needed

# =============================================================================
# PostgreSQL Database
# =============================================================================
POSTGRES_USER=mem0
POSTGRES_PASSWORD=mem0secret
POSTGRES_DB=mem0

# =============================================================================
# MCP Server Port
# =============================================================================
MCP_PORT=8050

# =============================================================================
# LLM Provider for Embeddings
# Options: ollama, openai, openrouter
# =============================================================================
LLM_PROVIDER=ollama

# For Ollama (local, free):
# Make sure Ollama is running on your host machine
# Models to pull: ollama pull llama3.2 && ollama pull nomic-embed-text
LLM_BASE_URL=http://host.docker.internal:11434
LLM_CHOICE=llama3.2
EMBEDDING_MODEL_CHOICE=nomic-embed-text

# For OpenAI (cloud, paid):
# LLM_PROVIDER=openai
# LLM_BASE_URL=https://api.openai.com/v1
# LLM_API_KEY=sk-your-openai-api-key
# LLM_CHOICE=gpt-4o-mini
# EMBEDDING_MODEL_CHOICE=text-embedding-3-small

# For OpenRouter (cloud, various models):
# LLM_PROVIDER=openrouter
# LLM_BASE_URL=https://openrouter.ai/api/v1
# LLM_API_KEY=sk-or-your-openrouter-key
# LLM_CHOICE=anthropic/claude-3-haiku
# EMBEDDING_MODEL_CHOICE=openai/text-embedding-3-small
